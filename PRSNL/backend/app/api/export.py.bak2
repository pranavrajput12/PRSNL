"""
Export API endpoints for PRSNL data
"""
from fastapi import APIRouter, Depends, HTTPException, Response
from typing import List, Optional, Dict, Any
from datetime import datetime, date
from uuid import UUID
import json
import csv
import io
# Security imports will be added when authentication is implemented
from app.db.database import get_db_connection
from app.models.schemas import Item
import logging

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/export", tags=["export"])


@router.post("/json")
async def export_json(
    item_ids: Optional[List[UUID]] = None,
    tags: Optional[List[str]] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    conn=Depends(get_db_connection)
) -> Response:
    """
    Export items as JSON with optional filters
    """
    try:
        # Build query
        query = """
            SELECT DISTINCT i.*, 
                   array_agg(DISTINCT t.name) FILTER (WHERE t.name IS NOT NULL) as tags,
                   array_agg(DISTINCT jsonb_build_object(
                       'id', a.id,
                       'type', a.type,
                       'file_path', a.file_path,
                       'file_name', a.file_name,
                       'metadata', a.item_metadata
                   )) FILTER (WHERE a.id IS NOT NULL) as attachments
            FROM items i
            LEFT JOIN item_tags it ON i.id = it.item_id
            LEFT JOIN tags t ON it.tag_id = t.id
            LEFT JOIN attachments a ON i.id = a.item_id
            WHERE 1=1
        """
        params = []
        param_count = 0
        
        # Apply filters
        if item_ids:
            param_count += 1
            query += f" AND i.id = ANY(${param_count})"
            params.append(item_ids)
            
        if tags:
            param_count += 1
            query += f" AND t.name = ANY(${param_count})"
            params.append(tags)
            
        if start_date:
            param_count += 1
            query += f" AND i.created_at >= ${param_count}"
            params.append(start_date)
            
        if end_date:
            param_count += 1
            query += f" AND i.created_at <= ${param_count}"
            params.append(end_date)
            
        query += " GROUP BY i.id ORDER BY i.created_at DESC"
        
        rows = await conn.fetch(query, *params)
        
        # Format data
        items = []
        for row in rows:
            item_dict = dict(row)
            # Convert UUID to string for JSON serialization
            item_dict['id'] = str(item_dict['id'])
            # Convert datetime to ISO format
            for key in ['created_at', 'updated_at', 'captured_at']:
                if item_dict.get(key):
                    item_dict[key] = item_dict[key].isoformat()
            items.append(item_dict)
        
        # Create export data
        export_data = {
            "export_date": datetime.now().isoformat(),
            "version": "1.0",
            "total_items": len(items),
            "items": items
        }
        
        # Return as downloadable JSON
        json_content = json.dumps(export_data, indent=2)
        return Response(
            content=json_content,
            media_type="application/json",
            headers={
                "Content-Disposition": f"attachment; filename=prsnl_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            }
        )
        
    except Exception as e:
        logger.error(f"Export JSON failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/csv")
async def export_csv(
    item_ids: Optional[List[UUID]] = None,
    tags: Optional[List[str]] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    conn=Depends(get_db_connection)
) -> Response:
    """
    Export items as CSV with optional filters
    """
    try:
        # Build query (similar to JSON export)
        query = """
            SELECT i.id, i.title, i.url, i.type, i.summary, 
                   i.created_at, i.captured_at,
                   string_agg(DISTINCT t.name, ', ') as tags
            FROM items i
            LEFT JOIN item_tags it ON i.id = it.item_id
            LEFT JOIN tags t ON it.tag_id = t.id
            WHERE 1=1
        """
        params = []
        param_count = 0
        
        # Apply filters (same as JSON)
        if item_ids:
            param_count += 1
            query += f" AND i.id = ANY(${param_count})"
            params.append(item_ids)
            
        if tags:
            param_count += 1
            query += f" AND t.name = ANY(${param_count})"
            params.append(tags)
            
        if start_date:
            param_count += 1
            query += f" AND i.created_at >= ${param_count}"
            params.append(start_date)
            
        if end_date:
            param_count += 1
            query += f" AND i.created_at <= ${param_count}"
            params.append(end_date)
            
        query += " GROUP BY i.id ORDER BY i.created_at DESC"
        
        rows = await conn.fetch(query, *params)
        
        # Create CSV
        output = io.StringIO()
        writer = csv.DictWriter(
            output,
            fieldnames=['id', 'title', 'url', 'type', 'summary', 'tags', 'created_at', 'captured_at']
        )
        writer.writeheader()
        
        for row in rows:
            writer.writerow({
                'id': str(row['id']),
                'title': row['title'],
                'url': row['url'],
                'type': row['type'],
                'summary': row['summary'],
                'tags': row['tags'] or '',
                'created_at': row['created_at'].isoformat() if row['created_at'] else '',
                'captured_at': row['captured_at'].isoformat() if row['captured_at'] else ''
            })
        
        # Return as downloadable CSV
        csv_content = output.getvalue()
        return Response(
            content=csv_content,
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename=prsnl_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            }
        )
        
    except Exception as e:
        logger.error(f"Export CSV failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/markdown")
async def export_markdown(
    item_ids: Optional[List[UUID]] = None,
    tags: Optional[List[str]] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    group_by_tag: bool = False,
    conn=Depends(get_db_connection)
) -> Response:
    """
    Export items as Markdown with optional filters and grouping
    """
    try:
        # Get items (similar query to JSON export)
        query = """
            SELECT i.*, 
                   array_agg(DISTINCT t.name) FILTER (WHERE t.name IS NOT NULL) as tags
            FROM items i
            LEFT JOIN item_tags it ON i.id = it.item_id
            LEFT JOIN tags t ON it.tag_id = t.id
            WHERE 1=1
        """
        params = []
        param_count = 0
        
        # Apply filters
        if item_ids:
            param_count += 1
            query += f" AND i.id = ANY(${param_count})"
            params.append(item_ids)
            
        if tags:
            param_count += 1
            query += f" AND t.name = ANY(${param_count})"
            params.append(tags)
            
        if start_date:
            param_count += 1
            query += f" AND i.created_at >= ${param_count}"
            params.append(start_date)
            
        if end_date:
            param_count += 1
            query += f" AND i.created_at <= ${param_count}"
            params.append(end_date)
            
        query += " GROUP BY i.id ORDER BY i.created_at DESC"
        
        rows = await conn.fetch(query, *params)
        
        # Create markdown content
        markdown_lines = [
            "# PRSNL Knowledge Base Export",
            f"\nExported on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Total items: {len(rows)}\n",
            "---\n"
        ]
        
        if group_by_tag:
            # Group items by tags
            tag_groups = {}
            untagged = []
            
            for row in rows:
                if row['tags']:
                    for tag in row['tags']:
                        if tag not in tag_groups:
                            tag_groups[tag] = []
                        tag_groups[tag].append(row)
                else:
                    untagged.append(row)
            
            # Write grouped content
            for tag in sorted(tag_groups.keys()):
                markdown_lines.append(f"## {tag}\n")
                for item in tag_groups[tag]:
                    markdown_lines.extend(_format_item_markdown(item))
                markdown_lines.append("")
            
            if untagged:
                markdown_lines.append("## Untagged\n")
                for item in untagged:
                    markdown_lines.extend(_format_item_markdown(item))
        else:
            # Write items chronologically
            for row in rows:
                markdown_lines.extend(_format_item_markdown(row))
        
        # Return as downloadable markdown
        markdown_content = "\n".join(markdown_lines)
        return Response(
            content=markdown_content,
            media_type="text/markdown",
            headers={
                "Content-Disposition": f"attachment; filename=prsnl_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            }
        )
        
    except Exception as e:
        logger.error(f"Export Markdown failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _format_item_markdown(item: Dict[str, Any]) -> List[str]:
    """Format a single item as markdown"""
    lines = []
    
    # Title and metadata
    lines.append(f"### {item['title']}\n")
    
    if item['url']:
        lines.append(f"**URL**: [{item['url']}]({item['url']})")
    
    lines.append(f"**Type**: {item['type']}")
    lines.append(f"**Captured**: {item['captured_at'].strftime('%Y-%m-%d %H:%M') if item['captured_at'] else 'Unknown'}")
    
    if item['tags']:
        lines.append(f"**Tags**: {', '.join(item['tags'])}")
    
    lines.append("")
    
    # Summary
    if item['summary']:
        lines.append(f"{item['summary']}\n")
    
    # Key points
    if item.get('key_points'):
        lines.append("**Key Points:**")
        for point in item['key_points']:
            lines.append(f"- {point}")
        lines.append("")
    
    lines.append("---\n")
    
    return lines